{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "COMPLETE_PROJECT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3GWWH9MlSyC"
      },
      "source": [
        "1. import toghter.py and call the next_block to completely load the entire dataset and all , train the model and then use it.\n",
        "2. import load_model.py and call the next_block to just use the saved model and directly use the model.predict but it takes a very large amount of time to output prediction..... the time more or less same as that of the training of the model....but training the model is at backend so preferably use the toghter.py file .\n",
        "3. The PROJECT folder has the two directories namely Audio_text and Only_text for multimodal and unimodal sentiment analysis respectively .\n",
        "4. Both the directories have thier own Datasets and Payments folder which contains the necessary datasets and (html+css+js) files requied.\n",
        "5. The paths of the datasets is given appropriately in the code only one needs to upload the PROJECT folder directly onto the MyDrive section.\n",
        "6.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLvAx20toKTK"
      },
      "source": [
        "!pip install keras\n",
        "!pip install nltk\n",
        "!pip install math\n",
        "!pip install flask-ngrok\n",
        "!pip install SpeechRecognition\n",
        "!apt install libasound2-dev portaudio19-dev\n",
        "!pip install PyAudio\n",
        "!pip install librosa\n",
        "!pip install wavio\n",
        "\n",
        "from flask import Flask, render_template\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu-gcI9FT8Kg"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VhE7EK2uWe9"
      },
      "source": [
        "#RUN THIS BLOCK OF THE NOTEBOOK TO WORK WITH OUR AUDIO AND  TEXT BASED SENTIMENTAL ANALYSIS\n",
        "\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from subprocess import run, PIPE\n",
        "from flask import Flask,render_template,logging,request,redirect,url_for\n",
        "import threading\n",
        "from google.colab import files\n",
        "from scipy.io import wavfile\n",
        "%cd /content/drive/MyDrive/PROJECT/Audio_text/Payment\n",
        "#import load_model\n",
        "import togther\n",
        "import librosa\n",
        "import numpy as np\n",
        "import time\n",
        "import pyaudio\n",
        "import speech_recognition as sr\n",
        "\n",
        "#(The setup for flask backend)\n",
        "app=Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "app.config['UPLOAD_FOLDER']=True\n",
        "app. config[\"SECRET_KEY\"] =True\n",
        "important=\"\"\n",
        "\n",
        "\n",
        "#(The google speech to text converter on received audio file)\n",
        "def STT(file):\n",
        "  def callback(recognizer , audio):\n",
        "    try:\n",
        "        input_string=recognizer.recognize_google(audio,language=\"en-SG\")\n",
        "        global important\n",
        "        important=input_string\n",
        "    except:\n",
        "        print(\"Opps didn't catch\")\n",
        "  r=sr.Recognizer()\n",
        "  m=sr.AudioFile(file)\n",
        "  with m as source:\n",
        "        r.dynamic_energy_treshold=True\n",
        "        r.adjust_for_ambient_noise(source,duration=5)\n",
        "        time.sleep(0.5)\n",
        "  stop_listening=r.listen_in_background(m,callback)\n",
        "  for _ in range(8):time.sleep(0.1) \n",
        "  stop_listening()\n",
        "  for i in range(5):time.sleep(0.1)\n",
        "\n",
        "\n",
        "#(This is the route to the homepage of the web display)\n",
        "@app.route(\"/\",methods=['GET'])\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "#(this method is called as a response to the submission of the form in the html code)\n",
        "@app.route(\"/output\", methods=['POST'])\n",
        "def upload_file():\n",
        "    file=request.files['file']\n",
        "    if file.filename != '':\n",
        "        file.save(file.filename)\n",
        "        y,sr=librosa.load(file.filename)\n",
        "        y = (np.iinfo(np.int32).max * (y/np.abs(y).max())).astype(np.int32)\n",
        "        wavfile.write(file.filename, sr, y)\n",
        "        STT(file.filename)\n",
        "        #emotion=load_model.next_block(file.filename,important)\n",
        "        emotion=togther.next_block(file.filename,important)\n",
        "        im = Image.open(\"/content/drive/MyDrive/PROJECT/Audio_text/Payment/static/\"+emotion+\".jfif\")\n",
        "        data = io.BytesIO()\n",
        "        im.save(data, \"JPEG\")\n",
        "        encoded_img_data = base64.b64encode(data.getvalue())\n",
        "        return render_template('index.html',transcript=important,emotion=emotion,image=encoded_img_data.decode('utf-8'))\n",
        "    return redirect(url_for('index',transcript=\"important\"))\n",
        "\n",
        "#(continously run the backend until we manually terminate the code)\n",
        "threading.Thread(target=app.run()).start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMAkU2j9pNvq"
      },
      "source": [
        "#RUN THIS BLOCK OF THE NOTEBOOK TO WORK WITH OUR ONLY THE TEXT BASED SENTIMENTAL ANALYSIS\n",
        "\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from subprocess import run, PIPE\n",
        "from flask import Flask,render_template,logging,request,redirect,url_for\n",
        "import threading\n",
        "from google.colab import files\n",
        "%cd /content/drive/MyDrive/PROJECT/Only_text/Payment\n",
        "\n",
        "import mylib\n",
        "app=Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "app.config['UPLOAD_FOLDER']=True\n",
        "app. config[\"SECRET_KEY\"] =True\n",
        "important=\"\"\n",
        "\n",
        "\n",
        "def STT(file):\n",
        "  import time\n",
        "  import pyaudio\n",
        "  import speech_recognition as sr\n",
        "  def callback(recognizer , audio):\n",
        "    try:\n",
        "        input_string=recognizer.recognize_google(audio,language=\"en-SG\")\n",
        "        global important\n",
        "        important=input_string\n",
        "    except:\n",
        "        print(\"Opps didn't catch\")\n",
        "  r=sr.Recognizer()\n",
        "  m=sr.AudioFile(file)\n",
        "  with m as source:\n",
        "        r.dynamic_energy_treshold=True\n",
        "        r.adjust_for_ambient_noise(source,duration=5)\n",
        "        time.sleep(0.5)\n",
        "  stop_listening=r.listen_in_background(m,callback)\n",
        "  for _ in range(8):time.sleep(0.1) \n",
        "  stop_listening()\n",
        "  for i in range(5):time.sleep(0.1)\n",
        "\n",
        "\n",
        "@app.route(\"/\",methods=['GET'])\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "\n",
        "@app.route(\"/output\", methods=['POST'])\n",
        "def upload_file():\n",
        "    file=request.files['file']\n",
        "    if file.filename != '':\n",
        "        file.save(file.filename)\n",
        "        STT(file.filename)\n",
        "        emotion=mylib.next_block(important)\n",
        "        im = Image.open(\"/content/drive/MyDrive/PROJECT/Only_text/Payment/static/\"+emotion+\".jfif\")\n",
        "        data = io.BytesIO()\n",
        "        im.save(data, \"JPEG\")\n",
        "        encoded_img_data = base64.b64encode(data.getvalue())\n",
        "        return render_template('index.html',transcript=important,emotion=emotion,image=encoded_img_data.decode('utf-8'))\n",
        "    return redirect(url_for('index',transcript=\"important\"))\n",
        "\n",
        "\n",
        "threading.Thread(target=app.run()).start()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}